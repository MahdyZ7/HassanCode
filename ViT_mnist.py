# -*- coding: utf-8 -*-
"""CIFAR VIT EDITED

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15gns9lJrwxF7blCMe3lUgvI6qbgjlmKw

# Image classification with Vision Transformer

**Author:** [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)<br>
**Date created:** 2021/01/18<br>
**Last modified:** 2021/01/18<br>
**Description:** Implementing the Vision Transformer (ViT) model for image classification.

## Introduction

This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)
model by Alexey Dosovitskiy et al. for image classification,
and demonstrates it on the CIFAR-100 dataset.
The ViT model applies the Transformer architecture with self-attention to sequences of
image patches, without using convolution layers.

This example requires TensorFlow 2.4 or higher, as well as
[TensorFlow Addons](https://www.tensorflow.org/addons/overview),
which can be installed using the following command:

```python
pip install -U tensorflow-addons
```

## Setup
"""

#pip install -U tensorflow-addons

import numpy as np
import tensorflow as tf
import os
from keras.src.engine import base_layer, data_adapter
from keras.src.engine.training import _disallow_inside_tf_function, _get_verbosity, flatten_metrics_in_order
from keras.src.utils import version_utils, tf_utils
from keras.src import callbacks as callbacks_module
from tensorflow import keras
from keras import layers
from tensorflow.keras.datasets import mnist

#import tensorflow_addons as tfa


"""## Prepare the data"""
num_classes = 10
input_shape = (28, 28, 1)

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape((-1, 28, 28, 1)).astype("float32") / 255.0
x_test = x_test.reshape((-1, 28, 28, 1)).astype("float32") / 255.0

print(f"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}")
print(f"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}")

"""## Configure the hyperparameters"""

learning_rate = 0.001
weight_decay = 0.0001
batch_size = 256
num_epochs = 50
image_size = 72  # We'll resize input images to this size
patch_size = 6  # Size of the patches to be extract from the input images
num_patches = (image_size // patch_size) ** 2 # Half: 72 #Normal: 144
projection_dim = 64
num_heads = 4
transformer_units = [
    projection_dim * 2,
    projection_dim,
]  # Size of the transformer layers
transformer_layers = 8
mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier

is_training=True

"""## Use data augmentation"""

data_augmentation = keras.Sequential(
    [
        layers.Normalization(),
        layers.Resizing(image_size, image_size),
        layers.RandomFlip("horizontal"),
        layers.RandomRotation(factor=0.02),
        layers.RandomZoom(
            height_factor=0.2, width_factor=0.2
        ),
    ],
    name="data_augmentation",
)
# Compute the mean and the variance of the training data for normalization.
data_augmentation.layers[0].adapt(x_train)

"""## Implement multilayer perceptron (MLP)"""


def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = layers.Dense(units, activation=tf.nn.gelu)(x)
        x = layers.Dropout(dropout_rate)(x)
    return x

"""## Implement patch creation as a layer"""


def tf_delete2_patches(tensor,indices):

    sub = list(range(tensor.shape[2]))

    popped = 0
    for index in indices:
      sub.pop(index - popped)
      popped += 1

    return tf.transpose(tf.gather(tf.transpose(tensor, perm=[0,2,1,3]), sub, axis=1), perm=[0,2,1,3])


class Patches(layers.Layer):
    def __init__(self, patch_size):
        super().__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding="VALID",
        )

        # EDIT
        # if not is_training:
        #   patches = tf_delete2_patches(patches, np.arange(0, patches.shape[2], 2))

        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches

"""Let's display patches for a sample image"""

import matplotlib.pyplot as plt

plt.figure(figsize=(4, 4))
image = x_train[np.random.choice(range(x_train.shape[0]))]
plt.imshow(image.astype("uint8"))
plt.axis("off")

resized_image = tf.image.resize(
    tf.convert_to_tensor([image]), size=(image_size, image_size)
)
patches = Patches(patch_size)(resized_image)
print(f"Image size: {image_size} X {image_size}")
print(f"Patch size: {patch_size} X {patch_size}")
print(f"Patches per image: {patches.shape[1]}")
print(f"Elements per patch: {patches.shape[-1]}")

n = int(np.sqrt(patches.shape[1]))
#EDITED
plt.figure(figsize=(4, 4)) # For half use: (2, 2); For full use (4, 4)
for i, patch in enumerate(patches[0]):
    ax = plt.subplot(n, n, i+1) #no of row, col # Use when even columns are NOT removed
    #ax = plt.subplot(12, 6, i + 1) #no of row, col # Use when even columns are removed
    patch_img = tf.reshape(patch, (patch_size, patch_size, 1))
    plt.imshow(patch_img.numpy().astype("uint8"))
    plt.axis("off")

"""## Implement the patch encoding layer

The `PatchEncoder` layer will linearly transform a patch by projecting it into a
vector of size `projection_dim`. In addition, it adds a learnable position
embedding to the projected vector.
"""


class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim):
        super().__init__()
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        encoded = self.projection(patch) + self.position_embedding(positions)
        return encoded

"""## Build the ViT model
`
The ViT model consists of multiple Transformer blocks,
which use the `layers.MultiHeadAttention` layer as a self-attention mechanism
applied to the sequence of patches. The Transformer blocks produce a
`[batch_size, num_patches, projection_dim]` tensor, which is processed via an
classifier head with softmax to produce the final class probabilities output.

Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),
which prepends a learnable embedding to the sequence of encoded patches to serve
as the image representation, all the outputs of the final Transformer block are
reshaped with `layers.Flatten()` and used as the image
representation input to the classifier head.
Note that the `layers.GlobalAveragePooling1D` layer
could also be used instead to aggregate the outputs of the Transformer block,
especially when the number of patches and the projection dimensions are large.
"""

def tf_delete2_vit_x1(tensor, indices):

    sub = list(range(tensor.shape[1]))

    popped = 0
    for index in indices:
      sub.pop(index - popped)
      popped += 1

    return tf.gather(tensor, sub, axis=1)

def create_vit_classifier():
    inputs = layers.Input(shape=input_shape)

    # Augment data.
    augmented = data_augmentation(inputs)

    # Create patches.
    patches = Patches(patch_size)(augmented)

    # Encode patches.
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

    # Create multiple layers of the Transformer block.
    for _ in range(transformer_layers):

        # Layer normalization 1.
        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)

        # Cut input in half for inference
        if not is_training:
          x1 = tf_delete2_vit_x1(x1, np.arange(0, x1.shape[1], 2))

        # Create a multi-head attention layer.
        attention_output = layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=projection_dim, dropout=0.1
        )(x1, x1)

        # Duplicate output back to full size
        if not is_training:
          attention_output = tf.gather(attention_output, np.repeat(np.arange(0, num_patches//2), 2), axis=1)

        # Skip connection 1.
        x2 = layers.Add()([attention_output, encoded_patches])

        # Layer normalization 2.
        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)

        # MLP.
        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)

        # Skip connection 2.
        encoded_patches = layers.Add()([x3, x2])

    # Create a [batch_size, projection_dim] tensor.
    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = layers.Flatten()(representation)
    representation = layers.Dropout(0.5)(representation)
    # Add MLP.
    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)
    # Classify outputs.
    logits = layers.Dense(num_classes)(features)
    # Create the Keras model.
    model = keras.Model(inputs=inputs, outputs=logits)
    return model

"""## Compile, train, and evaluate the mode"""

from time import time

def train(model):

    # optimizer = tfa.optimizers.AdamW(
    #     learning_rate=learning_rate, weight_decay=weight_decay
    # )

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate, weight_decay=weight_decay),
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
            keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
            keras.metrics.SparseTopKCategoricalAccuracy(5, name="top-5-accuracy"),
        ],
    )

    checkpoint_filepath = "tmp/checkpoint3"
    checkpoint_callback = keras.callbacks.ModelCheckpoint(
        checkpoint_filepath,
        monitor="val_accuracy",
        save_best_only=True,
        save_weights_only=True,
    )

    history = model.fit(
        x=x_train,
        y=y_train,
        batch_size=batch_size,
        epochs=num_epochs,
        validation_split=0.1,
        callbacks=[checkpoint_callback],
    )

    return history


def Evaluate(self, x=None, y=None, batch_size=None, verbose="auto", sample_weight=None,
        steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False,
        return_dict=False, **kwargs,):
    base_layer.keras_api_gauge.get_cell("evaluate").set(True)
    version_utils.disallow_legacy_graph("Model", "evaluate")
    self._assert_compile_was_called()
    self._check_call_args("evaluate")
    self._check_sample_weight_warning(x, sample_weight)
    _disallow_inside_tf_function("evaluate")
    use_cached_eval_dataset = kwargs.pop("_use_cached_eval_dataset", False)
    if kwargs:
        raise TypeError(f"Invalid keyword arguments: {list(kwargs.keys())}")

    if self.distribute_strategy._should_use_with_coordinator:
        self._cluster_coordinator = (
            tf.distribute.experimental.coordinator.ClusterCoordinator(
                self.distribute_strategy
            )
        )

    verbose = _get_verbosity(verbose, self.distribute_strategy)
    if self._pss_evaluation_shards:
        self._disallow_exact_eval_with_add_metrics()
    with self.distribute_strategy.scope():
        # Use cached evaluation data only when it's called in `Model.fit`
        if (
                use_cached_eval_dataset
                and getattr(self, "_eval_data_handler", None) is not None
        ):
            data_handler = self._eval_data_handler
        else:
            # Creates a `tf.data.Dataset` and handles batch and epoch
            # iteration.
            data_handler = data_adapter.get_data_handler(
                x=x,
                y=y,
                sample_weight=sample_weight,
                batch_size=batch_size,
                steps_per_epoch=steps,
                initial_epoch=0,
                epochs=1,
                max_queue_size=max_queue_size,
                workers=workers,
                use_multiprocessing=use_multiprocessing,
                model=self,
                steps_per_execution=self._steps_per_execution,
                pss_evaluation_shards=self._pss_evaluation_shards,
            )

        # Container that configures and calls `tf.keras.Callback`s.
        if not isinstance(callbacks, callbacks_module.CallbackList):
            callbacks = callbacks_module.CallbackList(
                callbacks,
                add_history=True,
                add_progbar=verbose != 0,
                model=self,
                verbose=verbose,
                epochs=1,
                steps=data_handler.inferred_steps,
            )

        # Initialize to prevent errors if 0 epochs are evaluated.
        logs = {}

        test_function_runner = self._get_test_function_runner(callbacks)
        self._test_counter.assign(0)
        callbacks.on_test_begin()
        for (
                _,
                dataset_or_iterator,
        ) in data_handler.enumerate_epochs():  # Single epoch.
            self.reset_metrics()
            with data_handler.catch_stop_iteration():
                for step in data_handler.steps():
                    with tf.profiler.experimental.Trace(
                            "test", step_num=step, _r=1
                    ):
                        callbacks.on_test_batch_begin(step)
                        logs = test_function_runner.run_step(
                            dataset_or_iterator,
                            data_handler,
                            step,
                            self._pss_evaluation_shards,
                        )

        logs = tf_utils.sync_to_numpy_or_python_type(logs)
        # Override with model metrics instead of last step logs
        if self._pss_evaluation_shards:
            logs = self._aggregate_exact_metrics(logs)
        else:
            logs = self._validate_and_get_metrics_result(logs)
        callbacks.on_test_end(logs=logs)

        if return_dict:
            return logs
        else:
            return flatten_metrics_in_order(logs, self.metrics_names)


def test(model):

    # optimizer = tfa.optimizers.AdamW(
    #     learning_rate=learning_rate, weight_decay=weight_decay
    # )

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate, weight_decay=weight_decay),
        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=[
            keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
            keras.metrics.SparseTopKCategoricalAccuracy(5, name="top-5-accuracy"),
        ],
    )

    checkpoint_filepath = "tmp/checkpoint3"
    reader = tf.train.load_checkpoint(checkpoint_filepath)
    shape_key = reader.get_variable_to_shape_map()
    for key in shape_key:
        print("Varable Name: {}, Shape: {}".format(key, shape_key[key]))
    # exit()
    model.load_weights(checkpoint_filepath)

    # Test
    _, accuracy, top_5_accuracy = Evaluate(model,x_test, y_test)
    testEndTime = time()

    print(f"Test accuracy: {round(accuracy * 100, 2)}%")
    print(f"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%")

# Train
# is_training=True
# vit_classifier = create_vit_classifier()
# train(vit_classifier)

#
# Average the time taken for 10 test runs
#

runs=1

# Normal test time
is_training=True
vit_classifier = create_vit_classifier()
testStartTime = time()
for x in range(runs):
  test(vit_classifier)

#record.timings(runs, 'timing_data_1.csv')
#record.record('cpu_data_1.csv')
avgFullMhaTestTimeSeconds = (time()-testStartTime)/runs

# Half-MHA test time
is_training=False
vit_classifier = create_vit_classifier()
testStartTime = time()
for x in range(runs):
  test(vit_classifier)

#record.timings(runs, 'timing_data_2.csv')
#record.record('cpu_data_2.csv')
avgHalfMhaTestTimeSeconds = (time()-testStartTime)/runs

print(f"Average Full-MHA test time in seconds: {avgFullMhaTestTimeSeconds}s")
print(f"Average Half-MHA test time in seconds: {avgHalfMhaTestTimeSeconds}s")

"""After 100 epochs, the ViT model achieves around 55% accuracy and
82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,
as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.

Note that the state of the art results reported in the
[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using
the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality
without pre-training, you can try to train the model for more epochs, use a larger number of
Transformer layers, resize the input images, change the patch size, or increase the projection dimensions.
Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices,
but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.
In practice, it's recommended to fine-tune a ViT model
that was pre-trained using a large, high-resolution dataset.
"""